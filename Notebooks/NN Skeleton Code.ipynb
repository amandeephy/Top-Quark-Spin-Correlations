{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L pt</th>\n",
       "      <th>Lbar pt</th>\n",
       "      <th>L eta</th>\n",
       "      <th>Lbar eta</th>\n",
       "      <th>Abs Delta Eta</th>\n",
       "      <th>Abs Delta Phi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>68.813210</td>\n",
       "      <td>27.302094</td>\n",
       "      <td>0.783428</td>\n",
       "      <td>0.082621</td>\n",
       "      <td>0.700806</td>\n",
       "      <td>2.790541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>53.816910</td>\n",
       "      <td>33.045055</td>\n",
       "      <td>-0.504254</td>\n",
       "      <td>0.294797</td>\n",
       "      <td>0.799050</td>\n",
       "      <td>0.797296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>32.939449</td>\n",
       "      <td>59.064556</td>\n",
       "      <td>-0.926941</td>\n",
       "      <td>1.881812</td>\n",
       "      <td>2.808753</td>\n",
       "      <td>1.635645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>38.570145</td>\n",
       "      <td>71.512238</td>\n",
       "      <td>0.044036</td>\n",
       "      <td>-0.361570</td>\n",
       "      <td>0.405607</td>\n",
       "      <td>1.633022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>43.054966</td>\n",
       "      <td>61.946774</td>\n",
       "      <td>0.135901</td>\n",
       "      <td>-0.311101</td>\n",
       "      <td>0.447003</td>\n",
       "      <td>0.544630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701589</td>\n",
       "      <td>66.935829</td>\n",
       "      <td>86.495033</td>\n",
       "      <td>2.153407</td>\n",
       "      <td>1.034294</td>\n",
       "      <td>1.119113</td>\n",
       "      <td>1.494943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701590</td>\n",
       "      <td>63.952545</td>\n",
       "      <td>40.639206</td>\n",
       "      <td>-0.625261</td>\n",
       "      <td>-1.788905</td>\n",
       "      <td>1.163644</td>\n",
       "      <td>2.543142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701591</td>\n",
       "      <td>218.240356</td>\n",
       "      <td>41.113899</td>\n",
       "      <td>-1.833916</td>\n",
       "      <td>-0.247579</td>\n",
       "      <td>1.586337</td>\n",
       "      <td>2.576784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701592</td>\n",
       "      <td>25.682802</td>\n",
       "      <td>34.545418</td>\n",
       "      <td>-1.336810</td>\n",
       "      <td>1.292909</td>\n",
       "      <td>2.629719</td>\n",
       "      <td>2.973686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701593</td>\n",
       "      <td>41.130291</td>\n",
       "      <td>48.387707</td>\n",
       "      <td>-0.876241</td>\n",
       "      <td>0.614441</td>\n",
       "      <td>1.490682</td>\n",
       "      <td>1.056447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>701594 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              L pt    Lbar pt     L eta  Lbar eta  Abs Delta Eta  \\\n",
       "0        68.813210  27.302094  0.783428  0.082621       0.700806   \n",
       "1        53.816910  33.045055 -0.504254  0.294797       0.799050   \n",
       "2        32.939449  59.064556 -0.926941  1.881812       2.808753   \n",
       "3        38.570145  71.512238  0.044036 -0.361570       0.405607   \n",
       "4        43.054966  61.946774  0.135901 -0.311101       0.447003   \n",
       "...            ...        ...       ...       ...            ...   \n",
       "701589   66.935829  86.495033  2.153407  1.034294       1.119113   \n",
       "701590   63.952545  40.639206 -0.625261 -1.788905       1.163644   \n",
       "701591  218.240356  41.113899 -1.833916 -0.247579       1.586337   \n",
       "701592   25.682802  34.545418 -1.336810  1.292909       2.629719   \n",
       "701593   41.130291  48.387707 -0.876241  0.614441       1.490682   \n",
       "\n",
       "        Abs Delta Phi  \n",
       "0            2.790541  \n",
       "1            0.797296  \n",
       "2            1.635645  \n",
       "3            1.633022  \n",
       "4            0.544630  \n",
       "...               ...  \n",
       "701589       1.494943  \n",
       "701590       2.543142  \n",
       "701591       2.576784  \n",
       "701592       2.973686  \n",
       "701593       1.056447  \n",
       "\n",
       "[701594 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open here your pickle files both the SUSY and the ttbar \n",
    "\n",
    "sig_df  = pd.read_pickle('SUSY_Mstop_175.pkl')\n",
    "bkg_df  = pd.read_pickle('ttbar.pkl')\n",
    "\n",
    "# Access the different columns of the data frame, say pt,eta etc ..\n",
    "bkg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the inputs by computing the z score\n",
    "# z score = x - mean / std_dev\n",
    "\n",
    "# To obtain the mean and std dev use :\n",
    "# mean      = df.mean()\n",
    "# std       = df.std() \n",
    "# and then \n",
    "# df_normed = df - mean /std \n",
    "\n",
    "# this operation acts on all rows at once, so no loop is needed\n",
    "# make sure you don't overwrite the original dataframe \n",
    "# Repeat for both SUSY (signal) and ttbar(backgroung) dataframe\n",
    "\n",
    "bkg_df   = bkg_df[1:len(sig_df)]\n",
    "\n",
    "sig_mean = sig_df.mean()\n",
    "sig_std  = sig_df.std()\n",
    "\n",
    "bkg_mean = bkg_df.mean()\n",
    "bkg_std  = bkg_df.std()\n",
    "\n",
    "sig_df   = (sig_df - bkg_mean)/ sig_std\n",
    "bkg_df   = (bkg_df - bkg_mean)/ bkg_std\n",
    "\n",
    "signal   = sig_df.values\n",
    "bkg      = bkg_df.values\n",
    "\n",
    "## Concatenate (join together) the 2 dataframes (sig_df_normed and bkg_df_normed) call this dataframe x\n",
    "x        = np.concatenate((signal,bkg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10067.,  8507.,  5967.,  3678.,  1787.,   904.,   336.,   111.,\n",
       "           31.,    14.]),\n",
       " array([-1.55514499, -0.88574594, -0.21634689,  0.45305216,  1.12245121,\n",
       "         1.79185026,  2.46124931,  3.13064836,  3.80004741,  4.46944645,\n",
       "         5.1388455 ]),\n",
       " <a list of 1 Patch objects>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(bkg_df['Abs Delta Eta'], histtype = 'step')\n",
    "plt.hist(sig_df['Abs Delta Eta'], histtype = 'step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a series of 1's as a label for signal \n",
    "# and 0's as a label for the background\n",
    "# Code provided, just uncomment\n",
    "\n",
    "# sig_label = np.ones(len(sig_df_normed))\n",
    "# bkg_label = np.zeros(len(bkg_df_normed))\n",
    "\n",
    "sig_label = np.ones(len(signal))\n",
    "bkg_label = np.zeros(len(bkg))\n",
    "\n",
    "# Concatenate (join together) the 2 dataframes (sig_label and bkg_label) call this dataframe y\n",
    "y         = np.concatenate((sig_label,bkg_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Naively you may assume \n",
    "\n",
    "# Cut x in two parts in a ratio 3:1, call one of them x_train and one x_test\n",
    "tf_length = int(0.75 * len(data))\n",
    "\n",
    "data_train = data[1: tf_length]\n",
    "data_test  = data[tf_length: len(data)]\n",
    "\n",
    "# Cut y in two parts in a ratio 3:1, call one of them y_train and one y_test\n",
    "label_train = label[1: tf_length]\n",
    "label_test  = label[tf_length : len(label)]\n",
    "\n",
    "# These are the training and testing datasets respectively\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15700"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_length = int(0.75 * len(x))\n",
    "label_train = y[1: tf_length]\n",
    "\n",
    "len(label_train[label_train == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#x_train, x_test = train_test_split(data, test_size=0.25)\n",
    "#y_train, y_test = train_test_split(label, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "47102\n",
      "15701\n"
     ]
    }
   ],
   "source": [
    "# Cut x in two parts in a ratio 3:1, call one of them x_train and one x_test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(x, test_size=0.25)\n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_test  = torch.from_numpy(x_test)\n",
    "\n",
    "# Cut y in two parts in a ratio 3:1, call one of them y_train and one y_test\n",
    "y_traindf, y_testdf = train_test_split(y, test_size=0.25)\n",
    "y_train = torch.from_numpy(y_traindf)\n",
    "y_test  = torch.from_numpy(y_testdf)\n",
    "print(type(y_train))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now shuffle the dataframes\n",
    "# Code provided, just uncomment\n",
    "idx = np.random.permutation(len(x_train))\n",
    "\n",
    "x_train = x_train[idx]\n",
    "y_train = y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model\n",
    "# Recall from your Spiral Classifier notebook :\n",
    "\n",
    "D = 6\n",
    "C = 2\n",
    "H = 10\n",
    "\n",
    "learning_rate = 1e-3\n",
    "lambda_l2     = 1e-5\n",
    "nepochs       = 150\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, C),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.values>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH]: 0, [LOSS]: 0.705940, [ACCURACY]: 0.500\n",
      "[EPOCH]: 1, [LOSS]: 0.705340, [ACCURACY]: 0.500\n",
      "[EPOCH]: 2, [LOSS]: 0.704744, [ACCURACY]: 0.500\n",
      "[EPOCH]: 3, [LOSS]: 0.704183, [ACCURACY]: 0.501\n",
      "[EPOCH]: 4, [LOSS]: 0.703635, [ACCURACY]: 0.502\n",
      "[EPOCH]: 5, [LOSS]: 0.703121, [ACCURACY]: 0.501\n",
      "[EPOCH]: 6, [LOSS]: 0.702618, [ACCURACY]: 0.502\n",
      "[EPOCH]: 7, [LOSS]: 0.702153, [ACCURACY]: 0.502\n",
      "[EPOCH]: 8, [LOSS]: 0.701698, [ACCURACY]: 0.501\n",
      "[EPOCH]: 9, [LOSS]: 0.701276, [ACCURACY]: 0.501\n",
      "[EPOCH]: 10, [LOSS]: 0.700868, [ACCURACY]: 0.501\n",
      "[EPOCH]: 11, [LOSS]: 0.700496, [ACCURACY]: 0.501\n",
      "[EPOCH]: 12, [LOSS]: 0.700130, [ACCURACY]: 0.501\n",
      "[EPOCH]: 13, [LOSS]: 0.699805, [ACCURACY]: 0.501\n",
      "[EPOCH]: 14, [LOSS]: 0.699486, [ACCURACY]: 0.501\n",
      "[EPOCH]: 15, [LOSS]: 0.699200, [ACCURACY]: 0.501\n",
      "[EPOCH]: 16, [LOSS]: 0.698927, [ACCURACY]: 0.501\n",
      "[EPOCH]: 17, [LOSS]: 0.698671, [ACCURACY]: 0.501\n",
      "[EPOCH]: 18, [LOSS]: 0.698432, [ACCURACY]: 0.501\n",
      "[EPOCH]: 19, [LOSS]: 0.698215, [ACCURACY]: 0.501\n",
      "[EPOCH]: 20, [LOSS]: 0.698008, [ACCURACY]: 0.500\n",
      "[EPOCH]: 21, [LOSS]: 0.697815, [ACCURACY]: 0.500\n",
      "[EPOCH]: 22, [LOSS]: 0.697640, [ACCURACY]: 0.500\n",
      "[EPOCH]: 23, [LOSS]: 0.697475, [ACCURACY]: 0.500\n",
      "[EPOCH]: 24, [LOSS]: 0.697313, [ACCURACY]: 0.500\n",
      "[EPOCH]: 25, [LOSS]: 0.697168, [ACCURACY]: 0.500\n",
      "[EPOCH]: 26, [LOSS]: 0.697035, [ACCURACY]: 0.501\n",
      "[EPOCH]: 27, [LOSS]: 0.696909, [ACCURACY]: 0.500\n",
      "[EPOCH]: 28, [LOSS]: 0.696788, [ACCURACY]: 0.501\n",
      "[EPOCH]: 29, [LOSS]: 0.696670, [ACCURACY]: 0.500\n",
      "[EPOCH]: 30, [LOSS]: 0.696565, [ACCURACY]: 0.500\n",
      "[EPOCH]: 31, [LOSS]: 0.696466, [ACCURACY]: 0.500\n",
      "[EPOCH]: 32, [LOSS]: 0.696370, [ACCURACY]: 0.499\n",
      "[EPOCH]: 33, [LOSS]: 0.696278, [ACCURACY]: 0.499\n",
      "[EPOCH]: 34, [LOSS]: 0.696186, [ACCURACY]: 0.499\n",
      "[EPOCH]: 35, [LOSS]: 0.696103, [ACCURACY]: 0.499\n",
      "[EPOCH]: 36, [LOSS]: 0.696019, [ACCURACY]: 0.499\n",
      "[EPOCH]: 37, [LOSS]: 0.695939, [ACCURACY]: 0.499\n",
      "[EPOCH]: 38, [LOSS]: 0.695867, [ACCURACY]: 0.499\n",
      "[EPOCH]: 39, [LOSS]: 0.695798, [ACCURACY]: 0.499\n",
      "[EPOCH]: 40, [LOSS]: 0.695726, [ACCURACY]: 0.499\n",
      "[EPOCH]: 41, [LOSS]: 0.695659, [ACCURACY]: 0.499\n",
      "[EPOCH]: 42, [LOSS]: 0.695594, [ACCURACY]: 0.499\n",
      "[EPOCH]: 43, [LOSS]: 0.695536, [ACCURACY]: 0.499\n",
      "[EPOCH]: 44, [LOSS]: 0.695468, [ACCURACY]: 0.498\n",
      "[EPOCH]: 45, [LOSS]: 0.695412, [ACCURACY]: 0.499\n",
      "[EPOCH]: 46, [LOSS]: 0.695358, [ACCURACY]: 0.499\n",
      "[EPOCH]: 47, [LOSS]: 0.695298, [ACCURACY]: 0.499\n",
      "[EPOCH]: 48, [LOSS]: 0.695249, [ACCURACY]: 0.499\n",
      "[EPOCH]: 49, [LOSS]: 0.695197, [ACCURACY]: 0.499\n",
      "[EPOCH]: 50, [LOSS]: 0.695145, [ACCURACY]: 0.499\n",
      "[EPOCH]: 51, [LOSS]: 0.695096, [ACCURACY]: 0.499\n",
      "[EPOCH]: 52, [LOSS]: 0.695052, [ACCURACY]: 0.498\n",
      "[EPOCH]: 53, [LOSS]: 0.695009, [ACCURACY]: 0.498\n",
      "[EPOCH]: 54, [LOSS]: 0.694966, [ACCURACY]: 0.498\n",
      "[EPOCH]: 55, [LOSS]: 0.694925, [ACCURACY]: 0.498\n",
      "[EPOCH]: 56, [LOSS]: 0.694879, [ACCURACY]: 0.499\n",
      "[EPOCH]: 57, [LOSS]: 0.694843, [ACCURACY]: 0.499\n",
      "[EPOCH]: 58, [LOSS]: 0.694805, [ACCURACY]: 0.499\n",
      "[EPOCH]: 59, [LOSS]: 0.694766, [ACCURACY]: 0.499\n",
      "[EPOCH]: 60, [LOSS]: 0.694730, [ACCURACY]: 0.498\n",
      "[EPOCH]: 61, [LOSS]: 0.694698, [ACCURACY]: 0.498\n",
      "[EPOCH]: 62, [LOSS]: 0.694665, [ACCURACY]: 0.497\n",
      "[EPOCH]: 63, [LOSS]: 0.694631, [ACCURACY]: 0.498\n",
      "[EPOCH]: 64, [LOSS]: 0.694599, [ACCURACY]: 0.498\n",
      "[EPOCH]: 65, [LOSS]: 0.694570, [ACCURACY]: 0.498\n",
      "[EPOCH]: 66, [LOSS]: 0.694538, [ACCURACY]: 0.498\n",
      "[EPOCH]: 67, [LOSS]: 0.694511, [ACCURACY]: 0.499\n",
      "[EPOCH]: 68, [LOSS]: 0.694483, [ACCURACY]: 0.499\n",
      "[EPOCH]: 69, [LOSS]: 0.694455, [ACCURACY]: 0.498\n",
      "[EPOCH]: 70, [LOSS]: 0.694429, [ACCURACY]: 0.499\n",
      "[EPOCH]: 71, [LOSS]: 0.694408, [ACCURACY]: 0.498\n",
      "[EPOCH]: 72, [LOSS]: 0.694381, [ACCURACY]: 0.499\n",
      "[EPOCH]: 73, [LOSS]: 0.694357, [ACCURACY]: 0.499\n",
      "[EPOCH]: 74, [LOSS]: 0.694334, [ACCURACY]: 0.499\n",
      "[EPOCH]: 75, [LOSS]: 0.694316, [ACCURACY]: 0.500\n",
      "[EPOCH]: 76, [LOSS]: 0.694293, [ACCURACY]: 0.499\n",
      "[EPOCH]: 77, [LOSS]: 0.694272, [ACCURACY]: 0.500\n",
      "[EPOCH]: 78, [LOSS]: 0.694249, [ACCURACY]: 0.500\n",
      "[EPOCH]: 79, [LOSS]: 0.694228, [ACCURACY]: 0.500\n",
      "[EPOCH]: 80, [LOSS]: 0.694212, [ACCURACY]: 0.500\n",
      "[EPOCH]: 81, [LOSS]: 0.694194, [ACCURACY]: 0.501\n",
      "[EPOCH]: 82, [LOSS]: 0.694176, [ACCURACY]: 0.500\n",
      "[EPOCH]: 83, [LOSS]: 0.694156, [ACCURACY]: 0.500\n",
      "[EPOCH]: 84, [LOSS]: 0.694140, [ACCURACY]: 0.500\n",
      "[EPOCH]: 85, [LOSS]: 0.694121, [ACCURACY]: 0.500\n",
      "[EPOCH]: 86, [LOSS]: 0.694103, [ACCURACY]: 0.500\n",
      "[EPOCH]: 87, [LOSS]: 0.694088, [ACCURACY]: 0.501\n",
      "[EPOCH]: 88, [LOSS]: 0.694071, [ACCURACY]: 0.500\n",
      "[EPOCH]: 89, [LOSS]: 0.694056, [ACCURACY]: 0.501\n",
      "[EPOCH]: 90, [LOSS]: 0.694042, [ACCURACY]: 0.501\n",
      "[EPOCH]: 91, [LOSS]: 0.694025, [ACCURACY]: 0.501\n",
      "[EPOCH]: 92, [LOSS]: 0.694018, [ACCURACY]: 0.501\n",
      "[EPOCH]: 93, [LOSS]: 0.694000, [ACCURACY]: 0.501\n",
      "[EPOCH]: 94, [LOSS]: 0.693989, [ACCURACY]: 0.502\n",
      "[EPOCH]: 95, [LOSS]: 0.693969, [ACCURACY]: 0.502\n",
      "[EPOCH]: 96, [LOSS]: 0.693954, [ACCURACY]: 0.502\n",
      "[EPOCH]: 97, [LOSS]: 0.693942, [ACCURACY]: 0.502\n",
      "[EPOCH]: 98, [LOSS]: 0.693928, [ACCURACY]: 0.501\n",
      "[EPOCH]: 99, [LOSS]: 0.693917, [ACCURACY]: 0.501\n",
      "[EPOCH]: 100, [LOSS]: 0.693907, [ACCURACY]: 0.502\n",
      "[EPOCH]: 101, [LOSS]: 0.693901, [ACCURACY]: 0.502\n",
      "[EPOCH]: 102, [LOSS]: 0.693886, [ACCURACY]: 0.502\n",
      "[EPOCH]: 103, [LOSS]: 0.693875, [ACCURACY]: 0.501\n",
      "[EPOCH]: 104, [LOSS]: 0.693861, [ACCURACY]: 0.501\n",
      "[EPOCH]: 105, [LOSS]: 0.693852, [ACCURACY]: 0.502\n",
      "[EPOCH]: 106, [LOSS]: 0.693839, [ACCURACY]: 0.502\n",
      "[EPOCH]: 107, [LOSS]: 0.693829, [ACCURACY]: 0.501\n",
      "[EPOCH]: 108, [LOSS]: 0.693824, [ACCURACY]: 0.502\n",
      "[EPOCH]: 109, [LOSS]: 0.693808, [ACCURACY]: 0.501\n",
      "[EPOCH]: 110, [LOSS]: 0.693799, [ACCURACY]: 0.501\n",
      "[EPOCH]: 111, [LOSS]: 0.693792, [ACCURACY]: 0.501\n",
      "[EPOCH]: 112, [LOSS]: 0.693785, [ACCURACY]: 0.502\n",
      "[EPOCH]: 113, [LOSS]: 0.693772, [ACCURACY]: 0.502\n",
      "[EPOCH]: 114, [LOSS]: 0.693760, [ACCURACY]: 0.502\n",
      "[EPOCH]: 115, [LOSS]: 0.693753, [ACCURACY]: 0.502\n",
      "[EPOCH]: 116, [LOSS]: 0.693746, [ACCURACY]: 0.502\n",
      "[EPOCH]: 117, [LOSS]: 0.693738, [ACCURACY]: 0.502\n",
      "[EPOCH]: 118, [LOSS]: 0.693729, [ACCURACY]: 0.502\n",
      "[EPOCH]: 119, [LOSS]: 0.693721, [ACCURACY]: 0.501\n",
      "[EPOCH]: 120, [LOSS]: 0.693714, [ACCURACY]: 0.501\n",
      "[EPOCH]: 121, [LOSS]: 0.693703, [ACCURACY]: 0.501\n",
      "[EPOCH]: 122, [LOSS]: 0.693698, [ACCURACY]: 0.502\n",
      "[EPOCH]: 123, [LOSS]: 0.693685, [ACCURACY]: 0.502\n",
      "[EPOCH]: 124, [LOSS]: 0.693677, [ACCURACY]: 0.502\n",
      "[EPOCH]: 125, [LOSS]: 0.693675, [ACCURACY]: 0.502\n",
      "[EPOCH]: 126, [LOSS]: 0.693668, [ACCURACY]: 0.502\n",
      "[EPOCH]: 127, [LOSS]: 0.693658, [ACCURACY]: 0.502\n",
      "[EPOCH]: 128, [LOSS]: 0.693653, [ACCURACY]: 0.502\n",
      "[EPOCH]: 129, [LOSS]: 0.693647, [ACCURACY]: 0.502\n",
      "[EPOCH]: 130, [LOSS]: 0.693640, [ACCURACY]: 0.502\n",
      "[EPOCH]: 131, [LOSS]: 0.693629, [ACCURACY]: 0.502\n",
      "[EPOCH]: 132, [LOSS]: 0.693624, [ACCURACY]: 0.502\n",
      "[EPOCH]: 133, [LOSS]: 0.693618, [ACCURACY]: 0.502\n",
      "[EPOCH]: 134, [LOSS]: 0.693615, [ACCURACY]: 0.502\n",
      "[EPOCH]: 135, [LOSS]: 0.693604, [ACCURACY]: 0.502\n",
      "[EPOCH]: 136, [LOSS]: 0.693598, [ACCURACY]: 0.502\n",
      "[EPOCH]: 137, [LOSS]: 0.693595, [ACCURACY]: 0.502\n",
      "[EPOCH]: 138, [LOSS]: 0.693586, [ACCURACY]: 0.502\n",
      "[EPOCH]: 139, [LOSS]: 0.693581, [ACCURACY]: 0.502\n",
      "[EPOCH]: 140, [LOSS]: 0.693568, [ACCURACY]: 0.501\n",
      "[EPOCH]: 141, [LOSS]: 0.693560, [ACCURACY]: 0.501\n",
      "[EPOCH]: 142, [LOSS]: 0.693554, [ACCURACY]: 0.501\n",
      "[EPOCH]: 143, [LOSS]: 0.693550, [ACCURACY]: 0.502\n",
      "[EPOCH]: 144, [LOSS]: 0.693543, [ACCURACY]: 0.502\n",
      "[EPOCH]: 145, [LOSS]: 0.693547, [ACCURACY]: 0.502\n",
      "[EPOCH]: 146, [LOSS]: 0.693541, [ACCURACY]: 0.502\n",
      "[EPOCH]: 147, [LOSS]: 0.693539, [ACCURACY]: 0.502\n",
      "[EPOCH]: 148, [LOSS]: 0.693532, [ACCURACY]: 0.503\n",
      "[EPOCH]: 149, [LOSS]: 0.693526, [ACCURACY]: 0.503\n"
     ]
    }
   ],
   "source": [
    "# nn package also has different loss functions.\n",
    "# We use cross entropy loss for our classification task\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# We use the optim package to apply \n",
    "# Stochastic Gradient Descent for our parameter updates\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# Training\n",
    "for t in range(nepochs):\n",
    "    \n",
    "    # This one line represents the forward pass\n",
    "    y_pred = model(x_train.float())\n",
    "    \n",
    "    # Compute the loss and accuracy    \n",
    "    loss   = loss_function(y_pred, y_train)\n",
    "    \n",
    "    # The predicted class is the one with maximum probability \n",
    "    # associated to it, hence we find the row wise maximas\n",
    "    \n",
    "    score, predicted = torch.max(y_pred,1)\n",
    "    \n",
    "    # Length of correct labels divided by total length is accuracy\n",
    "    acc    = (y_train == predicted).sum().float() / len(y_train)\n",
    "    \n",
    "    print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), acc))\n",
    "    \n",
    "    #display.clear_output(wait=True)\n",
    "    \n",
    "    # Zero the gradients before running\n",
    "    # the backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient\n",
    "    # of loss w.r.t our learnable params. \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your accuracy and verify it on the test data\n",
    "zero_pred = model(x_test.float()) [:,0]\n",
    "one_pred  = model(x_test.float()) [:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.000e+00, 5.000e+01, 3.126e+03, 1.031e+04, 2.006e+03, 1.840e+02,\n",
       "        1.800e+01, 1.000e+00, 1.000e+00, 2.000e+00]),\n",
       " array([-0.6931465 , -0.4684679 , -0.24378927, -0.01911064,  0.20556799,\n",
       "         0.43024662,  0.6549252 ,  0.87960386,  1.1042825 ,  1.3289611 ,\n",
       "         1.5536398 ], dtype=float32),\n",
       " <a list of 1 Patch objects>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(zero_pred.detach().numpy(), histtype = 'step' )\n",
    "plt.hist(one_pred.detach().numpy(), histtype = 'step' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0389e+00, -9.3967e-02, -1.7380e-01,  5.8254e-02, -1.1621e+00,\n",
       "         -8.4742e-01],\n",
       "        [-1.0365e+00, -4.7374e-01, -2.0042e+00, -2.1719e+00, -1.2828e+00,\n",
       "         -1.4004e+00],\n",
       "        [-9.6299e-01,  6.8967e-01,  3.7047e-01,  5.7046e-01, -1.2139e+00,\n",
       "         -6.5308e-02],\n",
       "        ...,\n",
       "        [-5.4390e-01,  3.7149e-01, -5.3353e-01, -3.2982e-01, -1.2098e+00,\n",
       "          2.0882e-01],\n",
       "        [-4.7847e-01,  4.8880e-01, -3.5708e-01,  1.3515e+00,  1.2795e+00,\n",
       "         -1.9509e+00],\n",
       "        [-9.4952e-01,  3.3045e-03,  2.7958e+00, -6.0449e-01,  4.0523e+00,\n",
       "          7.7609e-01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(sig_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
